{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1 - What is Data Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing packages and data\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data = pd.read_csv(location of data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages\n",
    "\n",
    "We'll learn more about these later:\n",
    "* Pandas: Data structures and analysis  \n",
    "* NumPy: Base n-dimensional array package  \n",
    "* SciPy: Fundamental library for scientific computing  \n",
    "* Matplotlib: Comprehensive 2D/3D plotting  \n",
    "* IPython: Enhanced interactive console  \n",
    "* Sympy: Symbolic mathematics  \n",
    "* Scikit-learn: Supervised/unsupervised learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# Calling functions\n",
    "\n",
    "def division(numerator, denominator):\n",
    "    result = float(numerator) / denominator\n",
    "    print result\n",
    "\n",
    "division(20, 10)\n",
    "division(10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cow', 'jumped', 'over', 'the', 'moon']\n"
     ]
    }
   ],
   "source": [
    "# Split string\n",
    "\n",
    "my_string = \"the cow jumped over the moon\"\n",
    "words = my_string.split()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many observations are in this dataframe - number of rows or number of items in an array\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# joins array\n",
    ".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2 - Research Design and Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prints first five lines\n",
    ".head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prints last five lines\n",
    ".tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to print column names\n",
    "for x in data.columns.values:\n",
    "    print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iloc vs loc vs ix\n",
    "\n",
    "loc - label based  \n",
    "iloc - position based  \n",
    "ix usually tries to behave like loc but falls back to behaving like iloc if the label is not in the index.  \n",
    "\n",
    "(row selection, column selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For NaN values\n",
    "\n",
    ".isnull()\n",
    ".dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How to get values from a specific column\n",
    "print data['Ozone'].mean()\n",
    "\n",
    "# How to get specific values for greater/less than\n",
    "print data[(data.Ozone > 31) & (data.Temp > 90)].head()\n",
    "\n",
    "# How to get a specific value\n",
    "print data[data.Month==6].Temp.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3 - Descriptive Statistics for Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods available include:  \n",
    ".min() - Compute minimum value  \n",
    ".max() - Compute maximum value  \n",
    ".mean() - Compute mean value  \n",
    ".median() - Compute median value  \n",
    ".mode() - Compute mode value(s)  \n",
    ".count() - Count the number of observations  \n",
    ".std() - Compute Standard Deviation  \n",
    ".var() - Compute variance  \n",
    ".describe() - Get a summary of the data  \n",
    ".corr() - Get correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get quartiles\n",
    "print df.quantile(.50) \n",
    "print df.quantile(0.25)\n",
    "print df.quantile(0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Box Plot\n",
    "df.plot(kind=\"box\")\n",
    "df['example1'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4 - Inferential Statistics for Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the shape of the DataFrame\n",
    "data.shape\n",
    "# it will give you this:\n",
    "(number of rows, number of columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "data.plot(kind='scatter', x='TV', y='Sales', ax=axs[0], figsize=(16, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The different kinds of plots:\n",
    "    \n",
    "* ‘bar’ or ‘barh’ for bar plots  \n",
    "* ‘hist’ for histogram  \n",
    "* ‘box’ for boxplot  \n",
    "* ‘kde’ or 'density' for density plots  \n",
    "* ‘area’ for area plots  \n",
    "* ‘scatter’ for scatter plots  \n",
    "* ‘hexbin’ for hexagonal bin plots  \n",
    "* ‘pie’ for pie plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the standard import if you're using \"formula notation\" (similar to R)\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# create a fitted model in one line\n",
    "# formula notiation is the equivalent to writting out our models such that 'outcome = predictor'\n",
    "# with the follwing syntax formula = 'outcome ~ predictor1 + predictor2 ... predictorN'\n",
    "lm = smf.ols(formula='Sales ~ TV', data=data).fit()\n",
    "\n",
    "# print the full summary\n",
    "# Full summary shows r-squared, p-value, skew, kurtosis, etc.\n",
    "lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You want a p-value that's less than 0.05 for it to be statistically significant.\n",
    "# The higher the r-squared, the better. \n",
    "\n",
    "# What does a 95% CI indicate? \n",
    "Answer: That if we repeated this study 100 times our point estimte would lie within that range 95 times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5 - Intro to Regression and Model Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression finds the best fit line:\n",
    "\n",
    "y = mx + b\n",
    "y = betas * X + alpha (+ error)\n",
    "\n",
    "Given a matrix X, their relative coefficients beta, and a y-intercept alpha, explain a dependent vector, y.\n",
    "\n",
    "_coefficient - a numerical or constant quantity placed before and multiplying the variable in an algebraic expression, such as 4 in 4x._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "====\n",
    "\n",
    "A linear regression works best when:\n",
    "* The data is normally distributed (but doesn't have to be)\n",
    "* The Xs significantly explain y (have low p-values)\n",
    "* The Xs are independent of each other (low multicollinearity)\n",
    "* The resulting values passes linear assumptions (dependent on problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# matplotlib and seaborn are used for graphs\n",
    "\n",
    "# create a matplotlib figure\n",
    "plt.figure()\n",
    "# generate a scatterplot inside the figure\n",
    "# plt.plot(x axis value, y axis value, marker for the graph)\n",
    "plt.plot(mammals.bodywt, mammals.brainwt, '.')\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lmplot returns a straight line\n",
    "# .lmplot(x, y, data)\n",
    "sns.lmplot('bodywt', 'brainwt', mammals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log transformations can be used to make highly skewed distributions less skewed. \n",
    "\n",
    "For example, as 64 = 26, then log2(64) = 6.  \n",
    "log10(1) = 0  \n",
    "log10(10) = 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Because both values are a log-log distribution, some math properties allow us to transform \n",
    "# them into normal distributions.\n",
    "\n",
    "# Create a new data set that converts all numeric variables into log10\n",
    "log_columns = ['bodywt', 'brainwt',]\n",
    "log_mammals = mammals.copy()\n",
    "log_mammals[log_columns] = log_mammals[log_columns].apply(np.log10)\n",
    "\n",
    "g = sns.lmplot('bodywt', 'brainwt', log_mammals)\n",
    "g.set_axis_labels( \"Log Body Weight\", \"Log Brain Weight\")\n",
    "\n",
    "# Even though we changed the way the data was shaped, this is still a linear result: \n",
    "# it's just linear in the log10 of the data, instead of in the data's natural state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fills all NaN values with whatever number you want\n",
    ".fillna(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn\n",
    "\n",
    "When modeling with sklearn, you'll use the following base principals.\n",
    "\n",
    "* All sklearn estimators (modeling classes) are based on this base estimator. This allows you to easily rotate through estimators without changing much code.  \n",
    "* All estimators take a matrix, X, either sparse or dense.  \n",
    "* Many estimators also take a vector, y, when working on a supervised machine learning problem. Regressions are supervised learning because we already have examples of y given X.  \n",
    "*  All estimators have parameters that can be set. This allows for customization and higher level of detail to the learning process. The parameters are appropriate to each estimator algorithm.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS (Ordinary Least Squares)\n",
    "\n",
    "Error is the difference between prediction and reality: the vertical distance between a real data point and the regression line. OLS is concerned with the squares of the errors. It tries to find the line going through the sample data that minimizes the sum of the squared errors.\n",
    "\n",
    "This approach assumes that the sample is representative of the population; that is, it assumes that the sample is unbiased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Betas\n",
    "\n",
    "Betas are the constants in the regression, the intercept and the slope. \n",
    "\n",
    "The intercept is the value of Y when X is 0.  \n",
    "The slope is also known as the regression coefficient.   \n",
    "Residual = the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit regression model \n",
    "Something = smf.ols(formula=' ', data=XX).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict value\n",
    "10**lm.predict(X_new.apply(np.log10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data.ColumnName.function\n",
    "# bike_data.weathersit.value_counts()\n",
    ".value_counts() - Number of times a value appears"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations\n",
    "To see the correlation matrix: .corr()\n",
    "\n",
    "But to get a heatmap use:  \n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)  \n",
    "correlations = bike_data[['temp', 'atemp', 'casual']].corr()  \n",
    "print correlations  \n",
    "print sns.heatmap(correlations, cmap=cmap)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################################################\n",
    "\n",
    "## Review over Citibike Data from Lesson 5\n",
    "\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 6 - Evaluating Model Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE)\n",
    "\n",
    "Mean squared error is the mean, or average, residual error in our model.\n",
    "\n",
    "To find MSE:\n",
    "\n",
    "1. Calculate the difference between each target y and the models predicted predicted value y-hat (this is how we determine the residual)  \n",
    "2. Square each residual.  \n",
    "3. Take the mean of the squared residual error.  \n",
    "\n",
    "#####\n",
    "\n",
    "To calculate using the function:  \n",
    "\n",
    "from sklearn import metrics  \n",
    "metrics.mean_squared_error(y, model.predict(X))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "from sklearn import metrics\n",
    "metrics.mean_squared_error([1, 2, 3, 4, 5], [5, 4, 3, 2, 1])\n",
    "# (4^2 + 2^2 + 0^2 + 2^2 + 4^2) / 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias\n",
    "\n",
    "When our error is described as biased, it means that the learner's prediction is consistently far away from the actual answer. This is a sign of poor sampling: perhaps the population is not well represented in the model, or other data needs to be collected. We'd prefer if the error was distributed more evenly across the model, even if that means it doesn't explain the sample as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "One approach data scientists use to account for bias is cross validation. The basic idea of cross validation is to generate several models based on different cross sections of the data, measure performance of each, and then take the mean performance. This technique is one way to swap bias error for generalized error in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold\n",
    "\n",
    "Split the data into k groups, train the data on all segments except one, and then test the performance on the remaining set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With precision, we're interested in producing a high amount of relevancy instead of irrelevancy. With recall, we're interesting in seeing how well a model returns specific data (literally, checking whether the model can recall what a class label looked like).\n",
    "\n",
    "Imagine predicting a marble color either green or red. There are 10 of each. If the model identifies 8 of the green marbles as green, the recall, or sensitivity, is .8. However, this says nothing about the number of red marbles that are also identified as green.\n",
    "\n",
    "Since the model predicted 8 of the green marbles as green, then precision would be 1, because all marbles predicted as green were in fact green. The precision of red marbles (assuming all red marbles were correct, and 2 green were predicted as red) would be roughly 0.833: 10 / (10 + 2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
